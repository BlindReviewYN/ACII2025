{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Wishful thinking 2nd Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate All Condition Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python pseudocode for generating combinations\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "# Define all parameter levels for the 2nd experiment\n",
    "domains = [\"Ball\", \"Football\"]\n",
    "roleplay_conditions = [\"None\", \"Full\", \"Full+Emo\"]\n",
    "good_bad_levels = [\"High bad\", \"None\", \"High good\"]\n",
    "uncertainty_levels = [\"50%\", \"probable\"]\n",
    "simulation_numbers = [10]\n",
    "extra_prompts = [\"None\", \"Hopeful\"]\n",
    "models = [\"GPT-4o\", \"o3-mini\", \"Claude 3.7\", \"Claude 3.7 Extended\", \n",
    "          \"gemini-flash 2.0\", \"gemini-flash 2.0 thinking\", \"deepseek v3\", \"deepseek R1\",\n",
    "          \"GPT-4.5\",\"Claude Opus 3.0\"]\n",
    "runs = list(range(10))\n",
    "\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "\n",
    "# Regular combinations\n",
    "for domain, roleplay, good_bad, uncertainty, sim_num, model, extra_prompt, run in itertools.product(\n",
    "    domains, roleplay_conditions, good_bad_levels, uncertainty_levels, \n",
    "    simulation_numbers, models, extra_prompts, runs):\n",
    "    if roleplay == \"Full+Emo\":  # Extra prompt only applies to roleplay conditions\n",
    "        all_combinations.append({\n",
    "            \"domain\": domain,\n",
    "            \"roleplay_condition\": roleplay,\n",
    "            \"good_bad_level\": good_bad,\n",
    "            \"uncertainty_level\": uncertainty,\n",
    "            \"simulation_number\": sim_num,\n",
    "            \"extra_prompt\": extra_prompt,\n",
    "            \"model\": model,\n",
    "            \"run\": run,\n",
    "            \"timestamp\": None,\n",
    "            \"response\": None,\n",
    "            \"thinking\": None,\n",
    "            \"answer\": None,\n",
    "            \"error_log\": None\n",
    "        })\n",
    "    else:\n",
    "        all_combinations.append({\n",
    "            \"domain\": domain,\n",
    "            \"roleplay_condition\": roleplay,\n",
    "            \"good_bad_level\": good_bad,\n",
    "            \"uncertainty_level\": uncertainty,\n",
    "            \"simulation_number\": sim_num,\n",
    "            \"extra_prompt\": \"None\",\n",
    "            \"model\": model,\n",
    "            \"run\": run,\n",
    "            \"timestamp\": None,\n",
    "            \"response\": None,\n",
    "            \"thinking\": None,\n",
    "            \"answer\": None,\n",
    "            \"error_log\": None\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'initial_data_2nd.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 159\u001b[0m\n\u001b[0;32m    157\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(generate_main_prompt, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    158\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(generate_system_prompt, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 159\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minitial_data_2nd.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\nutju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nutju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nutju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\nutju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\nutju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'initial_data_2nd.csv'"
     ]
    }
   ],
   "source": [
    "# Helper function to get appropriate outcome description by domain\n",
    "def get_outcome_description(domain):\n",
    "    if domain == \"Ball\":\n",
    "        return \"picking a blue ball\"\n",
    "    elif domain == \"Hurricane\":\n",
    "        return \"this hurricane hitting Florida\"\n",
    "    elif domain == \"Football\":\n",
    "        return \"the Sharks winning\"\n",
    "    elif domain == \"Quidditch\":\n",
    "        return \"the Griffins winning\"\n",
    "    return \"\"\n",
    "\n",
    "# Generate the system prompt based on roleplay condition\n",
    "def generate_system_prompt(row):\n",
    "    roleplay = row[\"roleplay_condition\"]\n",
    "    # Start with roleplay context (This will go into the system prompt)\n",
    "    if roleplay == \"None\":\n",
    "        rp_context = \"Instruction: When you answer the question about probability, you must provide your final answer as a number between 0 and 100 inside <answer> tags.\"\n",
    "    elif roleplay == \"Direct\":\n",
    "        rp_context = \"Instruction: When you answer the question about probability, you must provide your final answer as a number between 0 and 100 inside <answer> tags. Imagine you are in the following situation: \"\n",
    "    elif roleplay == 'Full':  # Full roleplay\n",
    "        rp_context = \"Instruction: When you answer the question about probability, you must provide your final answer as a number between 0 and 100 inside <answer> tags. You are Taylor, a 45-year-old professional living in Florida. From now on, you will answer the question from this character's perspective.\"\n",
    "    elif roleplay == 'Full+Emo':\n",
    "        rp_context = \"Instruction: When you answer the question about probability, you must provide your final answer as a number between 0 and 100 inside <answer> tags. You are Taylor, a 45-year-old professional living in Florida. From now on, you will answer the question from this character's perspective. You should consider potential human biases and nature when answering the question. You should let the character's emotions guide actions when appropriate.\"\n",
    "    return rp_context\n",
    "\n",
    "# Generate the full prompt based on row parameters\n",
    "def generate_main_prompt(row):\n",
    "    \"\"\"Generate the full prompt based on row parameters\"\"\"\n",
    "    domain = row[\"domain\"]\n",
    "    roleplay = row[\"roleplay_condition\"]\n",
    "    good_bad = row[\"good_bad_level\"]\n",
    "    uncertainty = row[\"uncertainty_level\"]\n",
    "    sim_num = row[\"simulation_number\"]\n",
    "    extra = row[\"extra_prompt\"]\n",
    "    \n",
    "    # Domain-specific setups, framings, and questions\n",
    "    if domain == \"Ball\":\n",
    "        if roleplay == \"None\":\n",
    "            setup = \"There is an urn with 100 balls that are either pink or blue, but the exact distribution is unknown. A ball is picked at random from the urn.\"\n",
    "            info_source = \"\"\n",
    "        else:\n",
    "            setup = \"You're a contestant on the game show 'Lucky Draw'. The host presents an urn with 100 balls that are either pink or blue, but the exact distribution is unknown. You draw one ball and hand it to the host without knowing the color.\"\n",
    "            info_source = \"The host informs you that \"\n",
    "            \n",
    "        # Good/bad framing for balls\n",
    "        if good_bad == \"High bad\":\n",
    "            framing = \"The rules state that you start with $10,000. If you draw a blue ball, you'll lose everything. If it's pink, you keep your money.\"\n",
    "        elif good_bad == \"Bad\":\n",
    "            framing = \"The rules state that you start with $100. If you draw a blue ball, you'll lose everything. If it's pink, you keep your money.\"\n",
    "        elif good_bad == \"None\":\n",
    "            framing = \"\"\n",
    "            if roleplay != \"None\":\n",
    "                framing = \"This is a tutorial round with no money involved.\"\n",
    "        elif good_bad == \"Good\":\n",
    "            framing = \"The rules state that if you draw a blue ball, you'll win $100. If it's pink, you win nothing.\"\n",
    "        elif good_bad == \"High good\":\n",
    "            framing = \"The rules state that if you draw a blue ball, you'll win $10,000. If it's pink, you win nothing.\"        \n",
    "        question = \"What do you think is the probability (0-100%) that the ball will be blue?\"\n",
    "    \n",
    "    elif domain == \"Hurricane\":\n",
    "        if roleplay == \"None\":\n",
    "            setup = \"A powerful hurricane is approaching the Florida coast. The region is under a hurricane warning.\"\n",
    "            info_source = \"\"\n",
    "        else:\n",
    "            setup = \"A powerful hurricane is rapidly approaching the Florida coast where you live. Your county is under a hurricane warning.\"\n",
    "            info_source = \"The latest meteorological report on TV states that \"\n",
    "            \n",
    "        # Good/bad framing for hurricane with decision already made\n",
    "        if good_bad == \"High bad\":\n",
    "            framing = \"You've decided to ride out the storm in your mobile home.\"\n",
    "        elif good_bad == \"Bad\":\n",
    "            framing = \"You've decided to stay in your house during the hurricane.\"\n",
    "        elif good_bad == \"None\":\n",
    "            framing = \"\"\n",
    "            if roleplay != \"None\":\n",
    "                framing = \"You have not decided whether to evacuate or stay.\"\n",
    "        elif good_bad == \"Good\":\n",
    "            framing = \"You've already evacuated to a shelter inland at a small financial cost.\"\n",
    "        elif good_bad == \"High good\":\n",
    "            framing = \"You've already evacuated to a hotel far inland at significant expense.\"\n",
    "        \n",
    "        question = \"What do you think is the probability (0-100%) that the hurricane will hit Florida?\"\n",
    "     \n",
    "    elif domain == \"Football\":\n",
    "        if roleplay == \"None\":\n",
    "            setup = \"A football match between the Orlando Sharks and Miami Dolphins is about to begin.\"\n",
    "            info_source = \"\"\n",
    "        else:\n",
    "            setup = \"You're at a sports bar watching the pre-game coverage for an upcoming football match between the Orlando Sharks and Miami Dolphins.\"\n",
    "            info_source = \"The sports analyst on TV mentions that \"\n",
    "            \n",
    "        # Good/bad framing for football with more realism\n",
    "        if good_bad == \"High bad\":\n",
    "            framing = \"You're a die-hard Dolphins fan and have been following them for years. This is the championship final, and winning would mean everything to you and your fellow fans.\"\n",
    "        elif good_bad == \"Bad\":\n",
    "            framing = \"You're supporting the Dolphins in this regular season game and are wearing their jersey.\"\n",
    "        elif good_bad == \"None\":\n",
    "            framing = \"\"\n",
    "            if roleplay != \"None\":\n",
    "                framing = \"You don't support either team and are just watching the game as a neutral observer.\"\n",
    "        elif good_bad == \"Good\":\n",
    "            framing = \"You're supporting the Sharks in this regular season game and are wearing their jersey.\"\n",
    "        elif good_bad == \"High good\":\n",
    "            framing = \"You're a die-hard Sharks fan and have been following them for years. This is the championship final, and winning would mean everything to you and your fellow fans.\"\n",
    "        \n",
    "        question = \"What do you think is the probability (0-100%) that the Sharks will win?\"\n",
    "            \n",
    "    elif domain == \"Quidditch\":\n",
    "        if roleplay == \"None\":\n",
    "            setup = \"A professional quidditch match between the Gainesville Griffins and the Tampa Thunderbirds is about to begin. The players are preparing to mount their brooms.\"\n",
    "            info_source = \"\"\n",
    "        else:\n",
    "            setup = \"You're attending a professional quidditch match between the Gainesville Griffins and the Tampa Thunderbirds. The crowd is buzzing with excitement as the players prepare to mount their brooms.\"\n",
    "            info_source = \"A quidditch commentator announces that \"\n",
    "            \n",
    "        # Good/bad framing for quidditch with more realism\n",
    "        if good_bad == \"High bad\":\n",
    "            framing = \"You're a passionate Thunderbirds supporter who's traveled across the state for this World Cup final match. You're decked out in team colors and have been following every game this season.\"\n",
    "        elif good_bad == \"Bad\":\n",
    "            framing = \"You're casually supporting the Thunderbirds today and bought a team pennant at the entrance.\"\n",
    "        elif good_bad == \"None\":\n",
    "            framing = \"\"\n",
    "            if roleplay != \"None\":\n",
    "                framing = \"You don't support either team and are just watching the match as a casual spectator.\"\n",
    "        elif good_bad == \"Good\":\n",
    "            framing = \"You're casually supporting the Griffins today and bought a team pennant at the entrance.\"\n",
    "        elif good_bad == \"High good\":\n",
    "            framing = \"You're a passionate Griffins supporter who's traveled across the state for this World Cup final match. You're decked out in team colors and have been following every game this season.\"\n",
    "        \n",
    "        question = \"What do you think is the probability (0-100%) that the Griffins will win?\"\n",
    "            \n",
    "    # Update information delivery with source\n",
    "    if uncertainty in [\"25%\", \"50%\", \"75%\"]:\n",
    "        if info_source:\n",
    "            info = f\"{info_source}based on {sim_num} simulation trials, the average probability of {get_outcome_description(domain)} is {uncertainty}.\"\n",
    "        else:\n",
    "            info = f\"Based on {sim_num} simulation trials, the average probability of {get_outcome_description(domain)} is {uncertainty}.\"\n",
    "    else:  # verbal uncertainty\n",
    "        if info_source:\n",
    "            info = f\"{info_source}based on {sim_num} simulation trials, {get_outcome_description(domain)} is {uncertainty}.\"\n",
    "        else:\n",
    "            info = f\"Based on {sim_num} simulation trials, {get_outcome_description(domain)} is {uncertainty}.\"        \n",
    "    \n",
    "    # Add extra prompt for hopeful condition (only applies to Ball domain)\n",
    "    extra_text = \"\"\n",
    "    if extra == \"Hopeful\":\n",
    "        extra_text = \"You feel really hopeful about the outcome. \"\n",
    "    \n",
    "    full_prompt = f\"{setup} {framing} {info} {extra_text}{question}\"\n",
    "    # Clean up any double spaces\n",
    "    full_prompt = ' '.join(full_prompt.split())\n",
    "    \n",
    "    return full_prompt\n",
    "\n",
    "# Example of applying to dataframe\n",
    "df[\"main_prompt\"] = df.apply(generate_main_prompt, axis=1)\n",
    "df[\"system_prompt\"] = df.apply(generate_system_prompt, axis=1)\n",
    "df.to_csv(\"initial_data_2nd.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preview some prompts for verification\n",
    "def preview_prompts(df, num_samples=5):\n",
    "    \"\"\"Preview a sample of generated prompts for verification\"\"\"\n",
    "    # Sample across different conditions\n",
    "    samples = []\n",
    "    \n",
    "    # Sample each domain\n",
    "    for domain in df[\"domain\"].unique():\n",
    "        domain_df = df[df[\"domain\"] == domain]\n",
    "        if len(domain_df) > 0:\n",
    "            samples.append(domain_df.iloc[0])\n",
    "    \n",
    "    # Sample different roleplay conditions\n",
    "    for rp in df[\"roleplay_condition\"].unique():\n",
    "        rp_df = df[df[\"roleplay_condition\"] == rp]\n",
    "        if len(rp_df) > 0 and len(samples) < num_samples:\n",
    "            samples.append(rp_df.iloc[0])\n",
    "    \n",
    "    # Sample different good/bad levels\n",
    "    for gb in df[\"good_bad_level\"].unique():\n",
    "        gb_df = df[df[\"good_bad_level\"] == gb]\n",
    "        if len(gb_df) > 0 and len(samples) < num_samples:\n",
    "            samples.append(gb_df.iloc[0])\n",
    "    \n",
    "    # Sample extra prompt condition\n",
    "    extra_df = df[df[\"extra_prompt\"] == \"Hopeful\"]\n",
    "    if len(extra_df) > 0:\n",
    "        samples.append(extra_df.iloc[0])\n",
    "    \n",
    "    # Deduplicate samples\n",
    "    unique_samples = []\n",
    "    for sample in samples:\n",
    "        if sample.name not in [s.name for s in unique_samples]:\n",
    "            unique_samples.append(sample)\n",
    "    \n",
    "    # Print previews\n",
    "    print(f\"Previewing {len(unique_samples)} sample prompts:\\n\")\n",
    "    for i, sample in enumerate(unique_samples[:num_samples]):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"Domain: {sample['domain']}\")\n",
    "        print(f\"Roleplay: {sample['roleplay_condition']}\")\n",
    "        print(f\"Good/Bad: {sample['good_bad_level']}\")\n",
    "        print(f\"Uncertainty: {sample['uncertainty_level']}\")\n",
    "        print(f\"Simulations: {sample['simulation_number']}\")\n",
    "        print(f\"Extra: {sample['extra_prompt']}\")\n",
    "        print(f\"Main Prompt: \\\"{sample['main_prompt']}\\\"\")\n",
    "        print(f\"System Prompt: \\\"{sample['system_prompt']}\\\"\\n\")\n",
    "\n",
    "# Call preview function to verify prompts before proceeding\n",
    "preview_prompts(df, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Models with Retry Logic and Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import openai\n",
    "import anthropic\n",
    "import re\n",
    "\n",
    "# Import other API clients as needed\n",
    "anthropic_client = anthropic.Anthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "openai_client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "openrouter_client = openai.OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "deepseek_client = openai.OpenAI(\n",
    "  base_url=\"https://api.deepseek.com\",\n",
    "  api_key=os.getenv(\"DEEPSEEK_API_KEY\")\n",
    ")\n",
    "\n",
    "model_mapping = {\n",
    "    \"GPT-4o\": \"gpt-4o\",\n",
    "    \"GPT-4.5\":\"gpt-4.5-preview-2025-02-27\",\n",
    "    \"o3-mini\": \"o3-mini\",\n",
    "    \"Claude 3.7\":\"claude-3-7-sonnet-20250219\",\n",
    "    \"Claude Opus 3.0\":\"claude-3-opus-20240229\",\n",
    "    \"gemini-flash 2.0\": \"google/gemini-2.0-flash-001\",\n",
    "    \"gemini-flash 2.0 thinking\": \"google/gemini-2.5-pro-preview-03-25\", #update to 2.5 since flash thinking doesn't seem to work\n",
    "    \"deepseek v3\": \"deepseek-chat\",\n",
    "    \"deepseek R1\": \"deepseek-reasoner\",\n",
    "}\n",
    "\n",
    "def query_model(row):\n",
    "    \"\"\"Query the specified model with retry logic\n",
    "    This function is used to query the specified model with retry logic.\n",
    "    It will try to query the model up to max_retries times.\n",
    "    If the model returns an error, it will wait for retry_delay seconds before retrying.\n",
    "    If the model returns a successful response, it will return the response and None.\n",
    "    If the model returns an error, it will return None and the error message.\n",
    "    Args:\n",
    "        row: A row from the dataframe containing the model, main_prompt, and system_prompt.\n",
    "    Returns:\n",
    "        A tuple containing the response, thinking content, and None if the response is successful, or None, None, and the error message if the response is not successful.\n",
    "    \"\"\"\n",
    "    model = row[\"model\"]\n",
    "    main_prompt = row[\"main_prompt\"]\n",
    "    system_prompt = row[\"system_prompt\"]\n",
    "    max_retries = 5\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if model in [\"GPT-4o\",\"GPT-4.5\"]:\n",
    "                # OpenAI API call\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_mapping[model],\n",
    "                    messages=[{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                              {\"role\": \"user\", \"content\": main_prompt}],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "\n",
    "                return response.choices[0].message.content, \"\", None\n",
    "            \n",
    "            elif model == \"o3-mini\":\n",
    "                # o3-mini (medium) API call\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_mapping[model],\n",
    "                    messages=[{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                              {\"role\": \"user\", \"content\": main_prompt}],\n",
    "                    reasoning_effort=\"medium\"\n",
    "                )   \n",
    "\n",
    "                return response.choices[0].message.content, \"\", None\n",
    "\n",
    "            elif model  in [\"Claude 3.7\",\"Claude Opus 3.0\"]:\n",
    "                # Anthropic API call\n",
    "\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_mapping[model],\n",
    "                    system=system_prompt,\n",
    "                    messages=[{\"role\": \"user\", \"content\": main_prompt}],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "                return response.content[0].text, \"\", None\n",
    "            \n",
    "            elif model == \"Claude 3.7 Extended\":\n",
    "                # Anthropic API call\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=\"claude-3-7-sonnet-20250219\",\n",
    "                    system=system_prompt,\n",
    "                    messages=[{\"role\": \"user\", \"content\": main_prompt}],\n",
    "                    temperature=1,\n",
    "                    thinking={\n",
    "                        \"type\": \"enabled\",\n",
    "                        \"budget_tokens\": 5000\n",
    "                    },\n",
    "                    max_tokens=10000\n",
    "                )\n",
    "                return response.content[1].text, response.content[0].thinking, None\n",
    "\n",
    "            elif model in [\"gemini-flash 2.0\", \"gemini-flash 2.0 thinking\"]:\n",
    "                response = openrouter_client.chat.completions.create(\n",
    "                    model=model_mapping[model],\n",
    "                    messages=[{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                              {\"role\": \"user\", \"content\": main_prompt}],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=8000\n",
    "                )   \n",
    "                return response.choices[0].message.content, \"\", None\n",
    "            elif model in [\"deepseek v3\", \"deepseek R1\"]:\n",
    "                response = deepseek_client.chat.completions.create(\n",
    "                    model=model_mapping[model],\n",
    "                    messages=[{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                              {\"role\": \"user\", \"content\": main_prompt}],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "                if model == \"deepseek R1\":\n",
    "                    return response.choices[0].message.content, response.choices[0].message.reasoning_content, None\n",
    "                else:\n",
    "                    return response.choices[0].message.content, \"\", None\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model: {model}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = f\"Attempt {attempt+1} failed: {str(e)}\"\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay * (2 ** attempt))  # Exponential backoff\n",
    "            else:\n",
    "                return None, None, error\n",
    "    \n",
    "def extract_answer(response):\n",
    "    \"\"\"Extract numerical answer from <answer> tags\"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "        \n",
    "    match = re.search(r'<answer>(.*?)</answer>', response)\n",
    "    if match:\n",
    "        try:\n",
    "            # Extract only the numerical value\n",
    "            return float(re.sub(r'[^\\d.]', '', match.group(1)))\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def process_model(model_name, df, time_delay=0.2):\n",
    "    \"\"\"Process all queries for a specific model with resume capability\n",
    "    Note: This function actually modifies the original dataframe in place. \n",
    "    \"\"\"\n",
    "    model_df = df[df[\"model\"] == model_name].copy()\n",
    "    \n",
    "    print(f\"Processing {len(model_df)} queries for {model_name}...\")\n",
    "    \n",
    "    # Count pending items\n",
    "    pending_count = len(model_df[(model_df[\"response\"].isna()) | (model_df[\"error_log\"].notna())])\n",
    "    print(f\"Found {pending_count} pending or failed queries to process\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    for i, (idx, row) in enumerate(model_df.iterrows()):\n",
    "        # Skip already processed rows with valid responses\n",
    "        if not pd.isna(row[\"response\"]) and pd.isna(row[\"error_log\"]):\n",
    "            continue\n",
    "        \n",
    "        # Update with timestamp\n",
    "        df.at[idx, \"timestamp\"] = datetime.now().isoformat()\n",
    "        \n",
    "        # Query model\n",
    "        response, thinking, error = query_model(row)\n",
    "        df.at[idx, \"response\"] = response\n",
    "        df.at[idx, \"thinking\"] = thinking  \n",
    "        df.at[idx, \"error_log\"] = error\n",
    "        \n",
    "        # Extract answer\n",
    "        if response:\n",
    "            answer = extract_answer(response)\n",
    "            df.at[idx, \"answer\"] = answer\n",
    "        \n",
    "        # Small delay to avoid rate limits\n",
    "        time.sleep(time_delay)\n",
    "        processed_count += 1\n",
    "        # Save progress every 100 rows\n",
    "        save_frequency = 60\n",
    "        if processed_count % save_frequency == 0:\n",
    "            save_path = f\"results/2nd experiment/wishful_thinking_2nd_exp_results_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            df.to_csv(save_path, index=False)\n",
    "            print(f\"Progress saved to {save_path}\")\n",
    "    \n",
    "    # Final save\n",
    "    save_path = f\"results/2nd experiment/wishful_thinking_2nd_exp_results_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"All processing complete. Results saved to {save_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the models with a simple hello prompt \n",
    "# Create a new dataframe with a simple hello prompt with all models and three columns for testing query_model()\n",
    "df_hello = pd.DataFrame({\n",
    "    \"model\": models,\n",
    "    \"main_prompt\": [\"Hello\"]*len(models),\n",
    "    \"system_prompt\": [\"You are a helpful assistant.\"]*len(models)\n",
    "})\n",
    "\n",
    "# Testing Models\n",
    "print(f'{df_hello[\"model\"][0]} {query_model(df_hello.iloc[0])}')\n",
    "print(f'{df_hello[\"model\"][1]} {query_model(df_hello.iloc[1])}')\n",
    "print(f'{df_hello[\"model\"][2]} {query_model(df_hello.iloc[2])}')\n",
    "print(f'{df_hello[\"model\"][3]} {query_model(df_hello.iloc[3])}')\n",
    "print(f'{df_hello[\"model\"][4]} {query_model(df_hello.iloc[4])}')\n",
    "print(f'{df_hello[\"model\"][5]} {query_model(df_hello.iloc[5])}')\n",
    "print(f'{df_hello[\"model\"][6]} {query_model(df_hello.iloc[6])}')\n",
    "print(f'{df_hello[\"model\"][7]} {query_model(df_hello.iloc[7])}')\n",
    "# Expensive models\n",
    "print(f'{df_hello[\"model\"][8]} {query_model(df_hello.iloc[8])}')\n",
    "print(f'{df_hello[\"model\"][9]} {query_model(df_hello.iloc[9])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent saved file for a specific model\n",
    "def get_latest_result_file(model_name):\n",
    "    files = glob.glob(f\"results/2nd experiment/wishful_thinking_2nd_exp_results_{model_name}_*.csv\")\n",
    "    if not files:\n",
    "        return None\n",
    "    return max(files, key=os.path.getctime)\n",
    "\n",
    "# We run one model at a time\n",
    "model_to_resume = models[7]  # Change to your model\n",
    "latest_file = get_latest_result_file(model_to_resume)\n",
    "latest_file = None\n",
    "if latest_file:\n",
    "    # Load the existing results\n",
    "    saved_df = pd.read_csv(latest_file)\n",
    "    print(f\"Loaded {len(saved_df)} rows from {latest_file}\")\n",
    "    \n",
    "    # To continue processing just this model\n",
    "    updated_df = process_model(model_to_resume, saved_df, time_delay=0)\n",
    "    \n",
    "else:\n",
    "    # Process from scratch\n",
    "    df = process_model(model_to_resume, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
